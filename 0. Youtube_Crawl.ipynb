{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db5d7526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a search query: 화이자\n",
      "Data saved to '화이자_2023-07-12_16-31-50.csv' file.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "driver = webdriver.Chrome('chromedriver', options=options)\n",
    "\n",
    "search_query = input(\"Enter a search query: \")\n",
    "\n",
    "driver.get(f\"https://www.youtube.com/results?search_query={search_query}\")\n",
    "\n",
    "driver.implicitly_wait(3)\n",
    "time.sleep(1.5)\n",
    "\n",
    "driver.execute_script('window.scrollTo(0,800)')\n",
    "time.sleep(3)\n",
    "\n",
    "last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "\n",
    "    last_height = new_height\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    try:\n",
    "        driver.find_element_by_css_selector('#dismiss-button > a').click()\n",
    "        time.sleep(1.5)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "html_source = driver.page_source\n",
    "soup = BeautifulSoup(html_source, 'html.parser')\n",
    "\n",
    "video_links = soup.find_all('a', {'class': 'yt-simple-endpoint style-scope ytd-video-renderer'})\n",
    "video_data = []\n",
    "\n",
    "for link in video_links:\n",
    "    video_url = 'https://www.youtube.com' + link['href']\n",
    "    \n",
    "    # Get video publication date\n",
    "    date_element = link.find_next('div', {'id': 'metadata-line'})\n",
    "    if date_element:\n",
    "        publication_date = date_element.get_text(strip=True).split('•')[-1].strip()\n",
    "    else:\n",
    "        publication_date = 'N/A'\n",
    "    \n",
    "    video_data.append({'Video URL': video_url, 'Publication Date': publication_date})\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Create filename with keyword and current time\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "filename = f\"{search_query}_{current_time}.csv\"\n",
    "\n",
    "# Save data to the CSV file\n",
    "fieldnames = ['Video URL', 'Publication Date']\n",
    "\n",
    "with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(video_data)\n",
    "\n",
    "print(f\"Data saved to '{filename}' file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7f850de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments scraped and saved to: crawl_pfizer.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_youtube_comments(urls):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    driver = webdriver.Chrome('chromedriver', options=options)\n",
    "    comments_data = []\n",
    "\n",
    "    for url in urls:\n",
    "        driver.get(url)\n",
    "        driver.implicitly_wait(1.8)\n",
    "        time.sleep(1)\n",
    "\n",
    "        driver.execute_script('window.scrollTo(0,800)')\n",
    "        time.sleep(1.8)\n",
    "\n",
    "        last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "            time.sleep(1)\n",
    "            new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "            time.sleep(1)\n",
    "\n",
    "            try:\n",
    "                driver.find_element_by_css_selector('#dismiss-button > a').click()\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        html_source = driver.page_source\n",
    "        soup = BeautifulSoup(html_source, 'html.parser')\n",
    "\n",
    "        id_list = soup.select('div#header-author > h3 > #author-text > span')\n",
    "        comment_list = soup.select('yt-formatted-string#content-text')\n",
    "        date_list = soup.select('yt-formatted-string.published-time-text')\n",
    "\n",
    "        for i in range(len(comment_list)):\n",
    "            temp_id = id_list[i].text\n",
    "            temp_id = temp_id.replace('\\n','').replace('\\t','').replace(' ','').strip()\n",
    "\n",
    "            temp_comment = comment_list[i].text\n",
    "            temp_comment = temp_comment.replace('\\n','').replace('\\t','').strip()\n",
    "\n",
    "            temp_date = date_list[i].text.strip()\n",
    "\n",
    "            comments_data.append({'URL': url, '아이디': temp_id, '댓글 내용': temp_comment, '게시날짜': temp_date})\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    youtube_df = pd.DataFrame(comments_data)\n",
    "    file_path = 'crawl_pfizer.csv'\n",
    "    youtube_df.to_csv(file_path, encoding='UTF-8-sig', index=False)\n",
    "    print(\"Comments scraped and saved to:\", file_path)\n",
    "\n",
    "urls = [   \n",
    "    'https://www.youtube.com/watch?v=QRogcfRpeKQ&pp=ygUJ7ZmU7J207J6Q',\n",
    "    'https://www.youtube.com/shorts/k_JiUh6o3No',\n",
    "    'https://www.youtube.com/shorts/XMCtSZM0Tg8',\n",
    "    'https://www.youtube.com/watch?v=kajrras76GE&pp=ygUJ7ZmU7J207J6Q',\n",
    "    'https://www.youtube.com/watch?v=BJtqCNz9Y1A&pp=ygUJ7ZmU7J207J6Q',\n",
    "    'https://www.youtube.com/watch?v=CXwBUT0HSyY&pp=ygUJ7ZmU7J207J6Q'\n",
    "]\n",
    "\n",
    "scrape_youtube_comments(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990f41cf",
   "metadata": {},
   "source": [
    "# 1~3년전 데이터만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8487e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추출된 데이터가 new 모더나 댓글 크롤링_extract.csv 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 업로드된 파일명\n",
    "uploaded_filename = 'new 모더나 댓글 크롤링.csv'\n",
    "\n",
    "# 새로운 파일명 생성\n",
    "new_filename = uploaded_filename.split('.')[0] + '_extract.csv'\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv(uploaded_filename)\n",
    "\n",
    "# '게시날짜' 열에서 '년 전'을 포함한 댓글만 선택\n",
    "df_extract = df[df['게시날짜'].str.contains('년 전')]\n",
    "\n",
    "# 새로운 파일로 저장 (인코딩: utf-8-sig)\n",
    "df_extract.to_csv(new_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"추출된 데이터가 {} 파일로 저장되었습니다.\".format(new_filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1f205d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_cpu] *",
   "language": "python",
   "name": "conda-env-tf_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
