{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f90e3a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "명사 추출이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from konlpy.tag import Okt, Komoran\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)' # E-mail제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+' # URL제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '([ㄱ-ㅎㅏ-ㅣ]+)'  # 한글 자음, 모음 제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '([^ ㄱ-힣]+)'  # 한글 외 제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '<[^>]*>'         # HTML 태그 제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '[^\\w\\s\\n]'         # 특수기호제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    text = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]','', string=text)\n",
    "    text = re.sub('\\n', ' ', string=text)\n",
    "    return text \n",
    "\n",
    "def clean_stopword(d):\n",
    "    return ' '.join([w for w in d.split() if w not in stopwords and len(w) > 3])\n",
    "\n",
    "def preprocessiong(text):\n",
    "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "    punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', } \n",
    "    for p in punct_mapping:\n",
    "        text = text.replace(p, punct_mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}\n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "okt = Okt()\n",
    "komoran = Komoran()\n",
    "\n",
    "def extract_nouns_from_sentence(sentence):\n",
    "    sentence = sentence.strip() \n",
    "    sentence = ''.join(ch for ch in sentence if ch.isalnum() or ch.isspace())  # Remove special characters\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    nouns = komoran.nouns(sentence) \n",
    "    return nouns\n",
    "\n",
    "def extract_nouns_from_comments(comments):\n",
    "    nouns_list = []\n",
    "    for comment in comments:\n",
    "        cleaned_comment = clean_text(comment)\n",
    "        preprocessed_comment = preprocessiong(cleaned_comment)\n",
    "        nouns = extract_nouns_from_sentence(preprocessed_comment)\n",
    "        if nouns:\n",
    "            nouns_list.append(' '.join(nouns))\n",
    "        else:\n",
    "            nouns_list.append(None)  \n",
    "    return nouns_list\n",
    "\n",
    "stopwords = []  # Add your stopwords here if needed\n",
    "\n",
    "uploaded_file_path = \"crawl_Janssen_extract_Sentiment_pos_data.csv\"\n",
    "output_file_path = os.path.splitext(uploaded_file_path)[0] + \"_noun.csv\"\n",
    "\n",
    "data = pd.read_csv(uploaded_file_path)\n",
    "\n",
    "nouns_list = extract_nouns_from_comments(data['댓글 내용'])\n",
    "\n",
    "data['추출된 명사'] = nouns_list\n",
    "\n",
    "nouns_data = pd.DataFrame({'추출된 명사': nouns_list})\n",
    "\n",
    "nouns_data.to_csv(output_file_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"명사 추출이 완료되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9f1d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# CSV 파일 로드\n",
    "df = pd.read_csv('vaccine_extract_Sentiment_noun_vaccine.csv')\n",
    "\n",
    "# 백신별로 단어 단위 키워드 추출 및 그룹화\n",
    "keyword_counts = {}\n",
    "for index, row in df.iterrows():\n",
    "    vaccine = row['백신']\n",
    "    keywords = re.findall(r'\\b\\w+\\b', str(row['추출된 명사']))  # 문자열로 변환하여 단어 단위로 추출\n",
    "    for keyword in keywords:\n",
    "        if len(keyword) > 1:  # 한 글자인 키워드 제외\n",
    "            if keyword not in keyword_counts:\n",
    "                keyword_counts[keyword] = {}\n",
    "            if vaccine not in keyword_counts[keyword]:\n",
    "                keyword_counts[keyword][vaccine] = 0\n",
    "            keyword_counts[keyword][vaccine] += 1\n",
    "\n",
    "# 새로운 CSV 파일로 저장\n",
    "result = pd.DataFrame(keyword_counts).fillna(0).T.reset_index().rename(columns={'index': '키워드'})\n",
    "result.to_csv('keyword_counts.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_cpu] *",
   "language": "python",
   "name": "conda-env-tf_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
